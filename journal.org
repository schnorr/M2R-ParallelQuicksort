# -*- coding: utf-8 -*-
#+STARTUP: overview indent inlineimages
#+TITLE:       Laboratory Notebook for a Multi-Threaded Version of Quicksort
#+AUTHOR:      Arnaud Legrand
#+LANGUAGE:    en
#+TAGS: IMPORTANT(i) TEST(t) DEPRECATED(d) noexport(n)

* Project Overview
This project aims at providing an efficient multi-threaded
implementation of the QuickSort algorithm on multi-core machines. This
document contains my attempts to evaluate the performance of an
implementation of such code.
* General Organization
** src/
This directory comprises the parallel implementation and a standard
Makefile to compile it.
** data/
This is where raw experimental data should go. Each directory entry
comprises a set of experiments and the directory name is based on the
machine name and on the date. For example:
#+begin_src sh :results output :exports both 
echo mkdir data/`hostname`_`date +%F`
#+end_src

#+RESULTS:
: mkdir data/sama_2014-10-13

* Typical usage
** Compilation
A simple makefile with various compilation options is provided in the
src/ directory. Compilation is thus done by running the following command:
#+begin_src sh :results output :exports both 
make -C src/
#+end_src

#+RESULTS:
: make: Entering directory '/home/alegrand/Work/Documents/Enseignements/M2R_Eval_Perf_13/M2R-ParallelQuicksort/src'
: cc   -g -Wall -Wshadow -Wcast-align -Waggregate-return -Wmissing-prototypes -Wmissing-declarations -Wstrict-prototypes -Wmissing-prototypes -Wmissing-declarations -Wmissing-noreturn -Wpointer-arith -Wwrite-strings -finline-functions -O0 -pthread -lrt -std=c99  -c -o parallelQuicksort.o parallelQuicksort.c
: cc   -g -Wall -Wshadow -Wcast-align -Waggregate-return -Wmissing-prototypes -Wmissing-declarations -Wstrict-prototypes -Wmissing-prototypes -Wmissing-declarations -Wmissing-noreturn -Wpointer-arith -Wwrite-strings -finline-functions -O0 -pthread -lrt -std=c99  parallelQuicksort.o  -o parallelQuicksort 
: make: Leaving directory '/home/alegrand/Work/Documents/Enseignements/M2R_Eval_Perf_13/M2R-ParallelQuicksort/src'

** Running the code
The code is quite simple at the moment and can be run in the following way:
#+begin_src
./src/parallelQuicksort [1000000]
#+end_src
When run, the code executes initializes an array of the size given in
argument (1000000 by default) with random integer values and sorts it
using:
1. a custom sequential implementation;
2. a custom parallel implementation;
3. the libc qsort function.
Times are reported in seconds.
* Experimental Reports
** 2014-10-13
*** Initial code design
- I obtained an initial implementation from
  http://sc12.supercomputing.org/hpceducator/PythonForParallelism/codes/parallelQuicksort.c.
  According to the header, the original author is Joshua Stough from
  Washington and Lee University. I hope he will not mind that I reuse
  this piece of code for educational purposes.
- Here is a typical first execution on my laptop (an Intel(R) Core(TM)
  i7 running a Debian with Linux 3.14.15):
  #+begin_src sh :results output :exports both 
    ./src/quicksort
  #+end_src

  #+RESULTS:
  : Sequential quicksort took: 0.231571 sec.
  : at loc 506315, 5.068226e-01 < 5.068269e-01 
  : Oops, lyst did not get sorted by parallelQuicksort.
  : Parallel quicksort took: 0.161259 sec.
  : Built-in qsort took: 0.241568 sec.

  Sweet, in my first attempt, it looks like this parallel version is
  indeed running faster than then sequential one. I have to say this
  warning message is stressing me a bit though.
- On smaller instances, the code would segfault. So I reindented the
  code and thanks to valgrind and gdb, I could find what was wrong. I
  also renamed the file so that compiling is more convenient. This
  fixed the previous warning message so now everything seems fine:
  #+begin_src sh :results output :exports both 
    ./src/parallelQuicksort
  #+end_src

  #+RESULTS:
  : Sequential quicksort took: 0.239347 sec.
  : Parallel quicksort took: 0.176365 sec.
  : Built-in quicksort took: 0.244716 sec.

*** First series of experiments
Let's try to see how the three algorithms behave when changing the 
array size. Since one measurement is not enough, I run the code 5
times in a row.
#+begin_src sh foo :results output :exports both :tangle scripts/run_benchmarking.sh
  OUTPUT_DIRECTORY=data/`hostname`_`date +%F`
  mkdir -p $OUTPUT_DIRECTORY
  OUTPUT_FILE=$OUTPUT_DIRECTORY/measurements_`date +%R`.txt

  touch $OUTPUT_FILE
  for i in 100 1000 10000 100000 1000000; do
      for rep in `seq 1 5`; do
          echo "Size: $i" >> $OUTPUT_FILE;
          ./src/parallelQuicksort $i >> $OUTPUT_FILE;
      done ;
  done
#+end_src
I obtained the following [[file:data/sama_2014-10-13/measurements_03:47.txt][output]].

*** A simple plot with R
Here is a simple script to parse the results:
#+begin_src perl :results output raw :exports both :tangle scripts/csv_quicksort_extractor.pl
  use strict;

  my($line);
  my($size);

  print "Size, Type, Time\n" ;
  while($line=<>) {
      chomp $line;
      if($line =~/^Size: ([\d\.]*)$/) {
          $size = $1;
          next;
      } 
      if($line =~/^(.*) quicksort.*: ([\d\.]*) sec.$/) {
          print "$size, \"$1\", $2\n" ;
          next;
      } 
  }
#+end_src

I can then simply parse my data with the following command:

#+begin_src sh :results output :exports both 
perl scripts/csv_quicksort_extractor.pl < data/sama_2014-10-13/measurements_03\:47.txt > data/sama_2014-10-13/measurements_03\:47.csv
#+end_src

#+RESULTS:

#+begin_src R :results output graphics :file data/sama_2014-10-13/measurements_03:47.png :exports both :width 600 :height 400 :session
  df <- read.csv("data/sama_2014-10-13/measurements_03:47.csv",header=T)
  plot(df$Size,df$Time,col=c("red","blue","green")[df$Type])
#+end_src

#+RESULTS:
[[file:data/sama_2014-10-13/measurements_03:47.png]]

Well, this is not particularly nice and some may not know/like R.
*** A simple plot with gnuplot
So let's try to parse in an other way and use gnuplot:

#+begin_src perl :results output raw :exports both :tangle scripts/csv_quicksort_extractor2.pl
  use strict;

  my($line);
  my($size);
  my($seq,$par,$libc);
  print "Size, Seq, Par, Libc\n" ;
  while($line=<>) {
      chomp $line;
      if($line =~/^Size: ([\d\.]*)$/) {
          $size = $1;
          next;
      } 
      if($line =~/^Sequential quicksort.*: ([\d\.]*) sec.$/) {
          $seq=$1; next;
      } 
      if($line =~/^Parallel quicksort.*: ([\d\.]*) sec.$/) {
          $par=$1; next;
      } 
      if($line =~/^Built-in quicksort.*: ([\d\.]*) sec.$/) {
          $libc=$1; 
          print "$size, $seq, $pqr, $libc\n";
          next;
      }
  }
#+end_src

#+begin_src sh :results output raw :exports both 
  FILENAME="data/sama_2014-10-13/measurements_03:47"
  perl scripts/csv_quicksort_extractor2.pl < "$FILENAME.txt" > "${FILENAME}_wide.csv"
  echo "
    set terminal png size 600,400 
    set output '${FILENAME}_wide.png'
    set datafile separator ','
    set key autotitle columnhead
    plot '${FILENAME}_wide.csv' using 1:2 with linespoints, '' using 1:3 with linespoints, '' using 1:4 with linespoints
  " | gnuplot
  echo [[file:${FILENAME}_wide.png]]
#+end_src

#+RESULTS:
[[file:data/sama_2014-10-13/measurements_03:47_wide.png]]

Well, I'm not sure it is nicer but I have lines. A first crude
analysis seems to reveal the the parallel version is worth it for
arrays larger than 400000.

** 2018-05-15 Analysis of blaise, draco6, hype4, tupi, knl2, orion3
*** Experimental design

Each student has run the following code in a different host.

#+begin_src sh foo :results output :exports both :tangle scripts/run_benchmarking.sh
  OUTPUT_DIRECTORY=data/`hostname`_`date +%F`
  mkdir -p $OUTPUT_DIRECTORY
  OUTPUT_FILE=$OUTPUT_DIRECTORY/measurements_`date +%R`.txt

  touch $OUTPUT_FILE
  for i in 100 1000 10000 100000 1000000 10000000 100000000; do
      for rep in `seq 1 5`; do
          echo "Size: $i" >> $OUTPUT_FILE;
          ./src/parallelQuicksort $i >> $OUTPUT_FILE;
      done ;
  done
#+end_src

*** Analysis

**** Preprocess TXT to single CSV with host as first column

#+begin_src shell :results output
for file in data/tupi_2018-05-15/measurements_10:57.txt data/blaise_2018-05-15/measurements_10:48.txt data/draco6_2018-05-15/measurements_10:50.txt data/hype4_2018-05-15/measurements_10:46.txt data/jequi_2018-05-15/measurements_10:47.txt data/knl2_2018-05-15/measurements_10:48.txt data/orion3_2018-05-15/measurements_10:48.txt; do
    NOME=$(echo $file | cut -d"/" -f2 | cut -d"_" -f1)
    CSV=$(basename $file .txt).csv
    perl scripts/csv_quicksort_extractor.pl < $file > $CSV
    cat $CSV | tail -n+2 | sed "s/^/$NOME,/" | sed "s/ //g"
done > cmp270_May15.csv
#+end_src

#+RESULTS:

**** Read data

#+begin_src R :results output :session :exports both
suppressMessages(library(tidyverse));
df <- read_csv("cmp270_May15.csv", col_names=FALSE, col_types=cols(
                                                   X1 = col_character(),
                                                   X2 = col_integer(),
                                                   X3 = col_character(),
                                                   X4 = col_double()
                                               )) %>%
    rename(Host = X1,
           Size = X2,
           Strategy = X3,
           Time = X4)
#+end_src

#+RESULTS:

**** Calculate average

#+begin_src R :results output :session :exports both
df %>%
    group_by(Host, Size, Strategy) %>%
    summarize(N=n(),
              Mean = mean(Time),
              SD = sd(Time),
              SE = 3*SD/sqrt(n())) %>%
    filter(Size == 100000000) -> p;
p;
#+end_src

#+RESULTS:
#+begin_example
# A tibble: 18 x 7
# Groups:   Host, Size [6]
     Host      Size   Strategy     N     Mean         SD         SE
    <chr>     <int>      <chr> <int>    <dbl>      <dbl>      <dbl>
 1 blaise 100000000   Built-in     5 27.44535 0.30598272 0.41051890
 2 blaise 100000000   Parallel     5 30.64253 1.31488330 1.76410107
 3 blaise 100000000 Sequential     5 26.44270 0.36203074 0.48571521
 4 draco6 100000000   Built-in     1 42.42106         NA         NA
 5 draco6 100000000   Parallel     1 38.15311         NA         NA
 6 draco6 100000000 Sequential     1 39.20502         NA         NA
 7  hype4 100000000   Built-in     5 31.93189 0.04884568 0.06553336
 8  hype4 100000000   Parallel     5 34.56596 0.49474631 0.66377183
 9  hype4 100000000 Sequential     5 31.61215 0.30135966 0.40431641
10  jequi 100000000   Built-in     5 25.12940 0.33461624 0.44893480
11  jequi 100000000   Parallel     5 14.36284 0.17969110 0.24108091
12  jequi 100000000 Sequential     5 24.30120 0.12516626 0.16792817
13 orion3 100000000   Built-in     5 39.58521 0.40100193 0.53800054
14 orion3 100000000   Parallel     5 37.51002 0.55632644 0.74639025
15 orion3 100000000 Sequential     5 36.00592 0.52749020 0.70770237
16   tupi 100000000   Built-in     5 45.90893 0.01085338 0.01456134
17   tupi 100000000   Parallel     5 27.42951 0.41426960 0.55580099
18   tupi 100000000 Sequential     5 45.32739 0.36440582 0.48890171
#+end_example

**** Plot 

***** Overview

#+begin_src R :results output graphics :file img/overview.png :exports both :width 600 :height 400 :session
library(ggplot2);
p %>%
    ggplot(aes(y=Mean, x=Strategy, color=Host)) +
    geom_point(size=2) +
    geom_errorbar(aes(ymin=Mean-SE, ymax=Mean+SE), width=0.1) +
    theme_bw(base_size=22)
#+end_src

#+RESULTS:
[[file:img/overview.png]]

***** Size 10M, all measurements

#+begin_src R :results output graphics :file img/size10M.png :exports both :width 600 :height 400 :session
library(ggplot2);
df %>%
    filter(Size == 10000000) %>%
    ggplot(aes(y=Time, x=Strategy, color=Host)) +
    geom_point(size=2) +
    theme_bw(base_size=22)
#+end_src

#+RESULTS:
[[file:img/size10M.png]]


**** All size, hosts, etc (averages)

#+begin_src R :results output graphics :file img/all_averages.png :exports both :width 600 :height 400 :session
library(ggplot2);
df %>%
    group_by(Host, Size, Strategy) %>%
    summarize(N=n(),
              Mean = mean(Time),
              SD = sd(Time),
              SE = 3*SD/sqrt(n())) %>%
#    filter(Size < 1000000) %>%
#    filter(Strategy!="Parallel") %>%
    ggplot(aes(y=Mean, x=Strategy, color=Host)) +
    geom_point(size=2) +
    theme_bw(base_size=16) +
    facet_wrap(~Size)
#+end_src

#+RESULTS:
[[file:img/all_averages.png]]
